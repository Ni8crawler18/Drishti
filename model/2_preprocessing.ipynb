{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dead966c-8a81-4366-9f9e-155489f624fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (2.1.0)\n",
      "Requirement already satisfied: nltk in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: indic-nlp-library in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (0.92)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: click in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: sphinx-argparse in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from indic-nlp-library) (0.5.2)\n",
      "Requirement already satisfied: sphinx-rtd-theme in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from indic-nlp-library) (3.0.2)\n",
      "Requirement already satisfied: morfessor in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from indic-nlp-library) (2.0.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sphinx>=5.1.0 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx-argparse->indic-nlp-library) (8.1.3)\n",
      "Requirement already satisfied: docutils>=0.19 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
      "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
      "Requirement already satisfied: Jinja2>=3.1 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.4)\n",
      "Requirement already satisfied: Pygments>=2.17 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
      "Requirement already satisfied: snowballstemmer>=2.2 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.13 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.14.0)\n",
      "Requirement already satisfied: alabaster>=0.7.14 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
      "Requirement already satisfied: imagesize>=1.3 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
      "Requirement already satisfied: requests>=2.30.0 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
      "Requirement already satisfied: packaging>=23.0 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ailab3/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install pandas numpy nltk indic-nlp-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6c2c14-a08e-495d-8e1e-60d595d958dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ailab3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ailab3/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ailab3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ailab3/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/ailab3/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset:\n",
      "                                category                       sub_category  \\\n",
      "0  Online and Social Media Related Crime  Cyber Bullying  Stalking  Sexting   \n",
      "1                 Online Financial Fraud                  Fraud CallVishing   \n",
      "2               Online Gambling  Betting           Online Gambling  Betting   \n",
      "3  Online and Social Media Related Crime                   Online Job Fraud   \n",
      "4                 Online Financial Fraud                  Fraud CallVishing   \n",
      "\n",
      "                                  crimeaditionalinfo  \\\n",
      "0  I had continue received random calls and abusi...   \n",
      "1  The above fraudster is continuously messaging ...   \n",
      "2  He is acting like a police and demanding for m...   \n",
      "3  In apna Job I have applied for job interview f...   \n",
      "4  I received a call from lady stating that she w...   \n",
      "\n",
      "                new_category                            new_sub_category  \n",
      "0  Women/Child Related Crime         ['Cyber Bullying/Stalking/Sexting']  \n",
      "1     Financial Fraud Crimes    ['Fraud Call/Vishing', 'Email Phishing']  \n",
      "2          Other Cyber Crime           ['Online Gambling/Betting Fraud']  \n",
      "3  Women/Child Related Crime  ['Online Job Fraud', 'Fraud Call/Vishing']  \n",
      "4     Financial Fraud Crimes    ['Fraud Call/Vishing', 'Email Phishing']  \n",
      "\n",
      "Test Dataset:\n",
      "                                    category  \\\n",
      "0  RapeGang Rape RGRSexually Abusive Content   \n",
      "1                     Online Financial Fraud   \n",
      "2             Cyber Attack/ Dependent Crimes   \n",
      "3                     Online Financial Fraud   \n",
      "4                      Any Other Cyber Crime   \n",
      "\n",
      "                           sub_category  \\\n",
      "0                                   NaN   \n",
      "1  DebitCredit Card FraudSim Swap Fraud   \n",
      "2                         SQL Injection   \n",
      "3                     Fraud CallVishing   \n",
      "4                                 Other   \n",
      "\n",
      "                                  crimeaditionalinfo  \\\n",
      "0  Sir namaskar  mein Ranjit Kumar PatraPaise neh...   \n",
      "1          KOTAK MAHINDRA BANK FRAUD\\r\\nFRAUD AMOUNT   \n",
      "2  The issue actually started when I got this ema...   \n",
      "3  I am amit kumar from karwi chitrakoot I am tot...   \n",
      "4  I have ordered  saree and  blouse from rinki s...   \n",
      "\n",
      "                new_category                               new_sub_category  \n",
      "0  Women/Child Related Crime                  ['Women/Child Related Crime']  \n",
      "1     Financial Fraud Crimes  ['Debit/Credit Card Fraud', 'SIM Swap Fraud']  \n",
      "2          Other Cyber Crime            ['Web application vulnerabilities']  \n",
      "3     Financial Fraud Crimes       ['Fraud Call/Vishing', 'Email Phishing']  \n",
      "4          Other Cyber Crime                          ['Other Cyber Crime']  \n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\n",
    "import nltk\n",
    "\n",
    "# NLTK Downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load datasets\n",
    "train_path = \"new_train.csv\"  # Update path\n",
    "test_path = \"new_test.csv\"    # Update path\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Display initial data\n",
    "print(\"Train Dataset:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest Dataset:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "213f3d4d-6af7-4bf3-b809-3d4c955ee535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Data After Dropping Categories:\n",
      "                                  crimeaditionalinfo  \\\n",
      "0  I had continue received random calls and abusi...   \n",
      "1  The above fraudster is continuously messaging ...   \n",
      "2  He is acting like a police and demanding for m...   \n",
      "3  In apna Job I have applied for job interview f...   \n",
      "4  I received a call from lady stating that she w...   \n",
      "\n",
      "                new_category                            new_sub_category  \n",
      "0  Women/Child Related Crime         ['Cyber Bullying/Stalking/Sexting']  \n",
      "1     Financial Fraud Crimes    ['Fraud Call/Vishing', 'Email Phishing']  \n",
      "2          Other Cyber Crime           ['Online Gambling/Betting Fraud']  \n",
      "3  Women/Child Related Crime  ['Online Job Fraud', 'Fraud Call/Vishing']  \n",
      "4     Financial Fraud Crimes    ['Fraud Call/Vishing', 'Email Phishing']  \n",
      "\n",
      "Test Data After Dropping Categories:\n",
      "                                  crimeaditionalinfo  \\\n",
      "0  Sir namaskar  mein Ranjit Kumar PatraPaise neh...   \n",
      "1          KOTAK MAHINDRA BANK FRAUD\\r\\nFRAUD AMOUNT   \n",
      "2  The issue actually started when I got this ema...   \n",
      "3  I am amit kumar from karwi chitrakoot I am tot...   \n",
      "4  I have ordered  saree and  blouse from rinki s...   \n",
      "\n",
      "                new_category                               new_sub_category  \n",
      "0  Women/Child Related Crime                  ['Women/Child Related Crime']  \n",
      "1     Financial Fraud Crimes  ['Debit/Credit Card Fraud', 'SIM Swap Fraud']  \n",
      "2          Other Cyber Crime            ['Web application vulnerabilities']  \n",
      "3     Financial Fraud Crimes       ['Fraud Call/Vishing', 'Email Phishing']  \n",
      "4          Other Cyber Crime                          ['Other Cyber Crime']  \n"
     ]
    }
   ],
   "source": [
    "# Drop 'category' and 'sub_category' columns if they exist\n",
    "for df in [train_df, test_df]:\n",
    "    df.drop(columns=['category', 'sub_category'], inplace=True, errors='ignore')\n",
    "\n",
    "print(\"\\nTrain Data After Dropping Categories:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest Data After Dropping Categories:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06edb7f6-808d-4579-9ac5-4518f8037532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transliterate Hinglish to English\n",
    "def transliterate_to_english(text):\n",
    "    try:\n",
    "        if isinstance(text, str):  # Ensure text is a string\n",
    "            return UnicodeIndicTransliterator.transliterate(text, 'hi', 'en')\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return text  # Fallback if transliteration fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c471061a-faab-4daf-bbba-4aab75d7e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Check if the input is a string\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove phone numbers and numeric strings\n",
    "    text = re.sub(r'\\b\\d{10}\\b', '', text)  # 10-digit phone numbers\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove standalone numbers\n",
    "    \n",
    "    # Remove special characters, punctuation, and excessive whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41b284a7-4409-4c59-91e6-57936d3c1e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stopwords and Lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function for stopword removal and lemmatization\n",
    "def preprocess_text(text):\n",
    "    try:\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and lemmatize\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        \n",
    "        return \" \".join(tokens)\n",
    "    except Exception as e:\n",
    "        return text  # Return original text if processing fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad5a0d06-b440-4337-b8bb-e3a56ff2a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing pipeline to 'crimeaditionalinfo' column\n",
    "for df in [train_df, test_df]:\n",
    "    if 'crimeaditionalinfo' in df.columns:\n",
    "        # Transliterate Hinglish/Hindi to English\n",
    "        df['processed_text'] = df['crimeaditionalinfo'].apply(transliterate_to_english)\n",
    "        \n",
    "        # Clean text\n",
    "        df['processed_text'] = df['processed_text'].apply(clean_text)\n",
    "        \n",
    "        # Stopword removal and lemmatization\n",
    "        df['processed_text'] = df['processed_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b022671-2277-4d26-bc49-f19bbd9ab836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Train Dataset:\n",
      "                                      processed_text  \\\n",
      "0  continue received random call abusive message ...   \n",
      "1  fraudster continuously messaging asking pay mo...   \n",
      "2  acting like police demanding money adding sect...   \n",
      "3  apna job applied job interview telecalling res...   \n",
      "4  received call lady stating send new phone vivo...   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0  [continue, received, random, call, abusive, me...  \n",
      "1  [fraudster, continuously, messaging, asking, p...  \n",
      "2  [acting, like, police, demanding, money, addin...  \n",
      "3  [apna, job, applied, job, interview, telecalli...  \n",
      "4  [received, call, lady, stating, send, new, pho...  \n",
      "\n",
      "Tokenized Test Dataset:\n",
      "                                      processed_text  \\\n",
      "0  sir namaskar mein ranjit kumar patrapaise nehi...   \n",
      "1             kotak mahindra bank fraud fraud amount   \n",
      "2  issue actually started got email first glance ...   \n",
      "3  amit kumar karwi chitrakoot totally depressed ...   \n",
      "4  ordered saree blouse rinki sur paid amount tak...   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0  [sir, namaskar, mein, ranjit, kumar, patrapais...  \n",
      "1      [kotak, mahindra, bank, fraud, fraud, amount]  \n",
      "2  [issue, actually, started, got, email, first, ...  \n",
      "3  [amit, kumar, karwi, chitrakoot, totally, depr...  \n",
      "4  [ordered, saree, blouse, rinki, sur, paid, amo...  \n"
     ]
    }
   ],
   "source": [
    "# Tokenize processed text\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(str(text))\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    if 'processed_text' in df.columns:\n",
    "        df['tokenized_text'] = df['processed_text'].apply(tokenize_text)\n",
    "\n",
    "# Display tokenized text\n",
    "print(\"\\nTokenized Train Dataset:\")\n",
    "print(train_df[['processed_text', 'tokenized_text']].head())\n",
    "\n",
    "print(\"\\nTokenized Test Dataset:\")\n",
    "print(test_df[['processed_text', 'tokenized_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "155491be-d536-4b56-90f3-4ecec22bdb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed datasets saved as 'processed_train.csv' and 'processed_test.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets\n",
    "train_df.to_csv(\"processed_train.csv\", index=False)\n",
    "test_df.to_csv(\"processed_test.csv\", index=False)\n",
    "\n",
    "print(\"\\nProcessed datasets saved as 'processed_train.csv' and 'processed_test.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
